{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "previous-combination",
   "metadata": {},
   "source": [
    "# mnist-explanation\n",
    "\n",
    "In this notebook, I try to implement explanation methods as described by G. Montavon et al. in their paper `Methods for Interpreting and Understanding Deep Neural Networks` [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "changing-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suitable-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = (0.1307, 0.3081)\n",
    "transforms = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize((mean,), (std,))\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prospective-updating",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "velvet-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset  = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "modified-worcester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Dimension: torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "fig, ax = plt.subplots(1,4)\n",
    "fig.suptitle('Training set samples')\n",
    "fig.set_size_inches(12,3)\n",
    "for i in range(images.shape[0]):\n",
    "    ax[i].imshow(images[i][0], cmap='gray')\n",
    "    ax[i].set_title('Groundtruth: {}'.format(labels[i].item()))\n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_yticks([])\n",
    "    \n",
    "print('Image Dimension: {}'.format(images[0][0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-credits",
   "metadata": {},
   "source": [
    "## Neural Network Implementation\n",
    "\n",
    "In their original paper, the researchers give advises on the kind of Deep Neural Network (DNN) to be used in order to maximise their explainability. We will follow the following advices:\n",
    " - Use as few fully-connected layers as needed to be accurate, and train these layers with dropout\n",
    " - Use sum-pooling layers abundantly, and prefer them to other types of pooling layers\n",
    " - In the linear layers (convolution and fully-connected), constrain biases to be zero or negative\n",
    "\n",
    "As such, I will be using a very simple CNN: 2 convolutional layers with associated average-pooling layers followed by 3 fully-connected layers, using the RELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "emerging-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstrainedConv2d(nn.Conv2d):\n",
    "    def forward(self, input):\n",
    "        return F.conv2d(input, self.weight, self.bias.clamp(self.bias.min().item(),0), self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "    \n",
    "class ConstrainedLinear(nn.Linear):\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight, self.bias.clamp(self.bias.min().item(),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "positive-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN(\n",
      "  (conv1): ConstrainedConv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): ConstrainedConv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): ConstrainedLinear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): ConstrainedLinear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): ConstrainedLinear(in_features=84, out_features=10, bias=True)\n",
      "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.conv1 = ConstrainedConv2d(1, 6, 3)\n",
    "        self.conv2 = ConstrainedConv2d(6, 16, 3)\n",
    "        self.fc1 = ConstrainedLinear(16 * 5 * 5, 120)\n",
    "        self.fc2 = ConstrainedLinear(120, 84)\n",
    "        self.fc3 = ConstrainedLinear(84, 10)\n",
    "        \n",
    "        self.pool = nn.AvgPool2d(2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(self.dropout(x)))\n",
    "        x = F.relu(self.fc2(self.dropout(x)))\n",
    "        x = self.fc3(self.dropout(x))\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "dnn = DNN()\n",
    "print(dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-metadata",
   "metadata": {},
   "source": [
    "I use the Adam optimiser (using PyTorch default values) together with the Categorical Cross-Entropy loss function, over 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "apart-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(dnn.parameters())\n",
    "cce = nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "model_path = './model.pt'\n",
    "force_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "configured-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# Is skipped if saved model exists and training is not forced through dedicated constant\n",
    "if os.path.isfile(model_path) and not force_train:\n",
    "    dnn = torch.load(model_path)\n",
    "else:\n",
    "    losses = np.zeros(epochs)\n",
    "    for epoch in trange(epochs, desc='Epochs'):\n",
    "        epoch_loss = 0.0\n",
    "        for data in tqdm(trainloader, desc='Epoch {}'.format(epoch)):\n",
    "            inputs, labels = data\n",
    "            opt.zero_grad()\n",
    "            outputs = dnn(inputs)\n",
    "            loss = cce(outputs, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_loss += loss.item()\n",
    "        losses[epoch] = epoch_loss\n",
    "        \n",
    "    torch.save(dnn, model_path)\n",
    "        \n",
    "    plt.plot(losses)\n",
    "    plt.title('Evolution of loss in every epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "expired-terrain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2128f523bc4e4db43b4516734eb1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 40000 test images: 99.11%\n"
     ]
    }
   ],
   "source": [
    "dnn.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(testloader, desc='Testing'):\n",
    "        images, labels = data\n",
    "        outputs = dnn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the {} test images: {:2}%'.format(total*4, 100.*correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-atlanta",
   "metadata": {},
   "source": [
    "With 99.11% accuracy over the test set, the result is judged satisfactory enough to be used in the explanation framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-money",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Grégoire Montavon, Wojciech Samek, Klaus-Robert Müller, Methods for interpreting and understanding deep neural networks, Digital Signal Processing, Volume 73, 2018, Pages 1-15, ISSN 1051-2004, https://doi.org/10.1016/j.dsp.2017.10.011. (https://www.sciencedirect.com/science/article/pii/S1051200417302385)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
